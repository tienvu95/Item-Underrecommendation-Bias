{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49fa4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 11:03:49.588271: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 11:03:49.655144: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import argparse\n",
    "import utility\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba3fa141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPR_RSP:\n",
    "\n",
    "    def __init__(self, sess, args, train_df, vali_df, item_genre, genre_error_weight,\n",
    "                 key_genre, item_genre_list, user_genre_count):\n",
    "        self.dataname = args.dataname\n",
    "\n",
    "        self.layers = eval(args.layers)\n",
    "\n",
    "        self.key_genre = key_genre\n",
    "        self.item_genre_list = item_genre_list\n",
    "        self.user_genre_count = user_genre_count\n",
    "\n",
    "        self.sess = sess\n",
    "        self.args = args\n",
    "\n",
    "        self.num_cols = len(train_df['item_id'].unique())\n",
    "        self.num_rows = len(train_df['user_id'].unique())\n",
    "\n",
    "        self.hidden_neuron = args.hidden_neuron\n",
    "        self.neg = args.neg\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.vali_df = vali_df\n",
    "        self.num_train = len(self.train_df)\n",
    "        self.num_vali = len(self.vali_df)\n",
    "\n",
    "        self.train_epoch = args.train_epoch\n",
    "        self.train_epoch_a = args.train_epoch_a\n",
    "        self.display_step = args.display_step\n",
    "\n",
    "        self.lr_r = args.lr_r  # learning rate\n",
    "        self.lr_a = args.lr_a  # learning rate\n",
    "\n",
    "        self.reg = args.reg  # regularization term trade-off\n",
    "        self.reg_s = args.reg_s\n",
    "\n",
    "        self.num_genre = args.num_genre\n",
    "        self.alpha = args.alpha\n",
    "        self.item_genre = item_genre\n",
    "        self.genre_error_weight = genre_error_weight\n",
    "\n",
    "        self.genre_count_list = []\n",
    "        for k in range(self.num_genre):\n",
    "            self.genre_count_list.append(np.sum(item_genre[:, k]))\n",
    "\n",
    "        print('**********DPR_RSP**********')\n",
    "        print(self.args)\n",
    "        self._prepare_model()\n",
    "\n",
    "    def loadmodel(self, saver, checkpoint_dir):\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def run(self):\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "        saver = tf.train.Saver([self.P, self.Q])\n",
    "        self.loadmodel(saver, \"./\"+self.dataname+\"/BPR_check_points\")\n",
    "\n",
    "        for epoch_itr in range(1, self.train_epoch + 1 + self.train_epoch_a):\n",
    "            self.train_model(epoch_itr)\n",
    "            if epoch_itr % self.display_step == 0:\n",
    "                self.test_model(epoch_itr)\n",
    "        return self.make_records()\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        with tf.name_scope(\"input_data\"):\n",
    "            self.user_input = tf.placeholder(tf.int32, shape=[None, 1], name=\"user_input\")\n",
    "            self.item_input_pos = tf.placeholder(tf.int32, shape=[None, 1], name=\"item_input_pos\")\n",
    "            self.item_input_neg = tf.placeholder(tf.int32, shape=[None, 1], name=\"item_input_neg\")\n",
    "\n",
    "            self.input_item_genre = tf.placeholder(dtype=tf.float32, shape=[None, self.num_genre]\n",
    "                                                   , name=\"input_item_genre\")\n",
    "            self.input_item_error_weight = tf.placeholder(dtype=tf.float32, shape=[None, 1]\n",
    "                                                          , name=\"input_item_error_weight\")\n",
    "\n",
    "        with tf.variable_scope(\"BPR\", reuse=tf.AUTO_REUSE):\n",
    "            self.P = tf.get_variable(name=\"P\",\n",
    "                                     initializer=tf.truncated_normal(shape=[self.num_rows, self.hidden_neuron], mean=0,\n",
    "                                                                     stddev=0.03), dtype=tf.float32)\n",
    "            self.Q = tf.get_variable(name=\"Q\",\n",
    "                                     initializer=tf.truncated_normal(shape=[self.num_cols, self.hidden_neuron], mean=0,\n",
    "                                                                     stddev=0.03), dtype=tf.float32)\n",
    "        para_r = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"BPR\")\n",
    "\n",
    "        with tf.variable_scope(\"Adversarial\", reuse=tf.AUTO_REUSE):\n",
    "            num_layer = len(self.layers)\n",
    "            adv_W = []\n",
    "            adv_b = []\n",
    "            for l in range(num_layer):\n",
    "                if l == 0:\n",
    "                    in_shape = 1\n",
    "                else:\n",
    "                    in_shape = self.layers[l - 1]\n",
    "                adv_W.append(tf.get_variable(name=\"adv_W\" + str(l),\n",
    "                                             initializer=tf.truncated_normal(shape=[in_shape, self.layers[l]],\n",
    "                                                                             mean=0, stddev=0.03), dtype=tf.float32))\n",
    "                adv_b.append(tf.get_variable(name=\"adv_b\" + str(l),\n",
    "                                             initializer=tf.truncated_normal(shape=[1, self.layers[l]],\n",
    "                                                                             mean=0, stddev=0.03), dtype=tf.float32))\n",
    "            adv_W_out = tf.get_variable(name=\"adv_W_out\",\n",
    "                                        initializer=tf.truncated_normal(shape=[self.layers[-1], self.num_genre],\n",
    "                                                                        mean=0, stddev=0.03), dtype=tf.float32)\n",
    "\n",
    "            adv_b_out = tf.get_variable(name=\"adv_b_out\",\n",
    "                                        initializer=tf.truncated_normal(shape=[1, self.num_genre],\n",
    "                                                                        mean=0, stddev=0.03), dtype=tf.float32)\n",
    "        para_a = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Adversarial\")\n",
    "\n",
    "\n",
    "        #dimension of P = (user_size * embedding_size (64 or 20 depends)), dimension of Q = (item_size * embedding_size (64 or 20 depends))\n",
    "        #dimension of user_input, item_input_neg, item_input_pos = batch size\n",
    "\n",
    "        p = tf.reduce_sum(tf.nn.embedding_lookup(self.P, self.user_input), 1)\n",
    "        q_neg = tf.reduce_sum(tf.nn.embedding_lookup(self.Q, self.item_input_neg), 1)\n",
    "        q_pos = tf.reduce_sum(tf.nn.embedding_lookup(self.Q, self.item_input_pos), 1)\n",
    "\n",
    "        #matrix multiplication\n",
    "\n",
    "        predict_pos = tf.reduce_sum(p * q_pos, 1) #torch.mul(p, q_pos).sum(dim=1)\n",
    "        predict_neg = tf.reduce_sum(p * q_neg, 1)\n",
    "\n",
    "        r_cost1 = tf.reduce_sum(tf.nn.softplus(-(predict_pos - predict_neg)))\n",
    "        r_cost2 = self.reg * 0.5 * (self.l2_norm(self.P) + self.l2_norm(self.Q))  # regularization term\n",
    "        pred = tf.matmul(self.P, tf.transpose(self.Q))\n",
    "        self.s_mean = tf.reduce_mean(pred, axis=1)\n",
    "        self.s_std = tf.keras.backend.std(pred, axis=1)\n",
    "        self.s_cost = tf.reduce_sum(tf.square(self.s_mean) + tf.square(self.s_std) - 2 * tf.log(self.s_std) - 1)\n",
    "        self.r_cost = r_cost1 + r_cost2 + self.reg_s * 0.5 * self.s_cost\n",
    "\n",
    "\n",
    "        #tf.shape(self.input_item_genre)[0] has shape = concat of pos_neg\n",
    "        adv_last = tf.reshape(tf.concat([predict_pos, predict_neg], axis=0), [tf.shape(self.input_item_genre)[0], 1])\n",
    "        for l in range(num_layer):\n",
    "            adv = tf.nn.relu(tf.matmul(adv_last, adv_W[l]) + adv_b[l])\n",
    "            adv_last = adv\n",
    "        self.adv_output = tf.nn.sigmoid(tf.matmul(adv_last, adv_W_out) + adv_b_out)\n",
    "        self.a_cost = tf.reduce_sum(tf.square(self.adv_output - self.input_item_genre) * self.input_item_error_weight)\n",
    "\n",
    "        self.all_cost = self.r_cost - self.alpha * self.a_cost  # the loss function\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\", reuse=tf.AUTO_REUSE):\n",
    "            self.r_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr_r).minimize(self.r_cost, var_list=para_r)\n",
    "            self.a_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr_a).minimize(self.a_cost, var_list=para_a)\n",
    "            self.all_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr_r).minimize(self.all_cost, var_list=para_r)\n",
    "\n",
    "    def train_model(self, itr):\n",
    "        NS_start_time = time.time() * 1000.0\n",
    "        epoch_r_cost = 0.0\n",
    "        epoch_s_cost = 0.0\n",
    "        epoch_s_mean = 0.0\n",
    "        epoch_s_std = 0.0\n",
    "        epoch_a_cost = 0.0\n",
    "        num_sample, user_list, item_pos_list, item_neg_list = utility.negative_sample(self.train_df, self.num_rows,\n",
    "                                                                                      self.num_cols, self.neg)\n",
    "        NS_end_time = time.time() * 1000.0\n",
    "\n",
    "        start_time = time.time() * 1000.0\n",
    "        num_batch = int(num_sample / float(self.batch_size)) + 1\n",
    "        random_idx = np.random.permutation(num_sample)\n",
    "        for i in range(num_batch):\n",
    "            # get the indices of the current batch\n",
    "            if i == num_batch - 1:\n",
    "                batch_idx = random_idx[i * self.batch_size:]\n",
    "            elif i < num_batch - 1:\n",
    "                batch_idx = random_idx[(i * self.batch_size):((i + 1) * self.batch_size)]\n",
    "\n",
    "            if itr > self.train_epoch:\n",
    "                random_idx_a = np.random.permutation(num_sample)\n",
    "                for j in range(num_batch):\n",
    "                    if j == num_batch - 1:\n",
    "                        batch_idx_a = random_idx_a[j * self.batch_size:]\n",
    "                    elif j < num_batch - 1:\n",
    "                        batch_idx_a = random_idx_a[(j * self.batch_size):((j + 1) * self.batch_size)]\n",
    "                    item_idx_list = ((item_pos_list[batch_idx_a, :]).reshape((len(batch_idx_a)))).tolist() \\\n",
    "                                    + ((item_neg_list[batch_idx_a, :]).reshape((len(batch_idx_a)))).tolist()\n",
    "                    _, tmp_a_cost = self.sess.run(  # do the optimization by the minibatch\n",
    "                        [self.a_optimizer, self.a_cost],\n",
    "                        feed_dict={self.user_input: user_list[batch_idx_a, :],\n",
    "                                   self.item_input_pos: item_pos_list[batch_idx_a, :],\n",
    "                                   self.item_input_neg: item_neg_list[batch_idx_a, :],\n",
    "                                   self.input_item_genre: self.item_genre[item_idx_list, :],\n",
    "                                   self.input_item_error_weight: self.genre_error_weight[item_idx_list, :]})\n",
    "                    epoch_a_cost += tmp_a_cost\n",
    "\n",
    "                item_idx_list = ((item_pos_list[batch_idx, :]).reshape((len(batch_idx)))).tolist() \\\n",
    "                                + ((item_neg_list[batch_idx, :]).reshape((len(batch_idx)))).tolist()\n",
    "                _, tmp_r_cost, tmp_s_cost, tmp_s_mean, tmp_s_std = self.sess.run(  # do the optimization by the minibatch\n",
    "                    [self.all_optimizer, self.all_cost, self.s_cost, self.s_mean, self.s_std],\n",
    "                    feed_dict={self.user_input: user_list[batch_idx, :],\n",
    "                               self.item_input_pos: item_pos_list[batch_idx, :],\n",
    "                               self.item_input_neg: item_neg_list[batch_idx, :],\n",
    "                               self.input_item_genre: self.item_genre[item_idx_list, :],\n",
    "                               self.input_item_error_weight: self.genre_error_weight[item_idx_list, :]})\n",
    "                epoch_r_cost += tmp_r_cost\n",
    "                epoch_s_mean += np.mean(tmp_s_mean)\n",
    "                epoch_s_std += np.mean(tmp_s_std)\n",
    "                epoch_s_cost += tmp_s_cost\n",
    "            else:\n",
    "                item_idx_list = ((item_pos_list[batch_idx, :]).reshape((len(batch_idx)))).tolist() \\\n",
    "                                + ((item_neg_list[batch_idx, :]).reshape((len(batch_idx)))).tolist()\n",
    "                _, tmp_r_cost, tmp_s_cost, tmp_s_mean, tmp_s_std = self.sess.run(  # do the optimization by the minibatch\n",
    "                    [self.r_optimizer, self.r_cost, self.s_cost, self.s_mean, self.s_std],\n",
    "                    feed_dict={self.user_input: user_list[batch_idx, :],\n",
    "                               self.item_input_pos: item_pos_list[batch_idx, :],\n",
    "                               self.item_input_neg: item_neg_list[batch_idx, :],\n",
    "                               self.input_item_genre: self.item_genre[item_idx_list, :],\n",
    "                               self.input_item_error_weight: self.genre_error_weight[item_idx_list, :]})\n",
    "                epoch_r_cost += tmp_r_cost\n",
    "                epoch_s_mean += np.mean(tmp_s_mean)\n",
    "                epoch_s_std += np.mean(tmp_s_std)\n",
    "                epoch_s_cost += tmp_s_cost\n",
    "        epoch_a_cost /= num_batch\n",
    "        if itr % self.display_step == 0:\n",
    "            print (\"Training //\", \"Epoch %d //\" % itr, \" Total r_cost = %.5f\" % epoch_r_cost,\n",
    "                   \" Total s_cost = %.5f\" % epoch_s_cost,\n",
    "                   \" Total s_mean = %.5f\" % epoch_s_mean,\n",
    "                   \" Total s_std = %.5f\" % epoch_s_std,\n",
    "                   \" Total a_cost = %.5f\" % epoch_a_cost,\n",
    "                   \"Training time : %d ms\" % (time.time() * 1000.0 - start_time),\n",
    "                   \"negative Sampling time : %d ms\" % (NS_end_time - NS_start_time),\n",
    "                   \"negative samples : %d\" % (num_sample))\n",
    "\n",
    "    def test_model(self, itr):  # calculate the cost and rmse of testing set in each epoch\n",
    "        if itr % self.display_step == 0:\n",
    "            start_time = time.time() * 1000.0\n",
    "            P, Q = self.sess.run([self.P, self.Q])\n",
    "            Rec = np.matmul(P, Q.T)\n",
    "\n",
    "            utility.test_model_all(Rec, self.vali_df, self.train_df)\n",
    "            utility.ranking_analysis(Rec, self.vali_df, self.train_df, self.key_genre, self.item_genre_list,\n",
    "                                     self.user_genre_count)\n",
    "            print(\n",
    "                \"Testing //\", \"Epoch %d //\" % itr,\n",
    "                \"Testing time : %d ms\" % (time.time() * 1000.0 - start_time))\n",
    "            print(\"=\" * 200)\n",
    "\n",
    "    def make_records(self):  # record all the results' details into files\n",
    "        P, Q = self.sess.run([self.P, self.Q])\n",
    "        Rec = np.matmul(P, Q.T)\n",
    "        [precision, recall, f_score, NDCG] = utility.test_model_all(Rec, self.vali_df, self.train_df)\n",
    "        return precision, recall, f_score, NDCG, Rec\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_norm(tensor):\n",
    "        return tf.reduce_sum(tf.square(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e742382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train_epoch TRAIN_EPOCH]\n",
      "                             [--train_epoch_a TRAIN_EPOCH_A]\n",
      "                             [--display_step DISPLAY_STEP] [--lr_r LR_R]\n",
      "                             [--lr_a LR_A] [--reg REG] [--reg_s REG_S]\n",
      "                             [--hidden_neuron HIDDEN_NEURON] [--n N]\n",
      "                             [--neg NEG] [--alpha ALPHA]\n",
      "                             [--batch_size BATCH_SIZE] [--layers [LAYERS]]\n",
      "                             [--dataname [DATANAME]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/vuhoang181/.local/share/jupyter/runtime/kernel-55a755e0-60ac-4de1-a6d3-3a45ad6b0367.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuhoang181/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='DPR_RSP')\n",
    "parser.add_argument('--train_epoch', type=int, default=0)\n",
    "parser.add_argument('--train_epoch_a', type=int, default=20)\n",
    "parser.add_argument('--display_step', type=int, default=1)\n",
    "parser.add_argument('--lr_r', type=float, default=0.01)\n",
    "parser.add_argument('--lr_a', type=float, default=0.005)\n",
    "parser.add_argument('--reg', type=float, default=0.1)\n",
    "parser.add_argument('--reg_s', type=float, default=30)\n",
    "parser.add_argument('--hidden_neuron', type=int, default=20)\n",
    "parser.add_argument('--n', type=int, default=1)\n",
    "parser.add_argument('--neg', type=int, default=5)\n",
    "parser.add_argument('--alpha', type=float, default=5000.0)\n",
    "parser.add_argument('--batch_size', type=int, default=1024)\n",
    "parser.add_argument('--layers', nargs='?', default='[50, 50, 50, 50]')\n",
    "parser.add_argument('--dataname', nargs='?', default='ml1m-6')\n",
    "args = parser.parse_args()\n",
    "\n",
    "dataname = 'ml1m-6'\n",
    "\n",
    "\n",
    "train_df = pickle.load(open('./' + dataname + '/training_df.pkl'))\n",
    "vali_df = pickle.load(open('./' + dataname + '/valiing_df.pkl'))  # for validation\n",
    "# vali_df = pickle.load(open('./' + dataname + '/testing_df.pkl'))  # for testing\n",
    "key_genre = pickle.load(open('./' + dataname + '/key_genre.pkl'))\n",
    "item_idd_genre_list = pickle.load(open('./' + dataname + '/item_idd_genre_list.pkl'))\n",
    "genre_item_vector = pickle.load(open('./' + dataname + '/genre_item_vector.pkl'))\n",
    "genre_count = pickle.load(open('./' + dataname + '/genre_count.pkl'))\n",
    "user_genre_count = pickle.load(open('./' + dataname + '/user_genre_count.pkl'))\n",
    "\n",
    "num_item = len(train_df['item_id'].unique())\n",
    "num_user = len(train_df['user_id'].unique())\n",
    "num_genre = len(key_genre)\n",
    "\n",
    "item_genre_list = []\n",
    "for u in range(num_item):\n",
    "    gl = item_idd_genre_list[u]\n",
    "    tmp = []\n",
    "    for g in gl:\n",
    "        if g in key_genre:\n",
    "            tmp.append(g)\n",
    "    item_genre_list.append(tmp)\n",
    "\n",
    "print('!' * 100)\n",
    "print('number of positive feedback: ' + str(len(train_df)))\n",
    "print('estimated number of training samples: ' + str(args.neg * len(train_df)))\n",
    "print('!' * 100)\n",
    "\n",
    "# genreate item_genre matrix\n",
    "item_genre = np.zeros((num_item, num_genre))\n",
    "for i in range(num_item):\n",
    "    gl = item_genre_list[i]\n",
    "    for k in range(num_genre):\n",
    "        if key_genre[k] in gl:\n",
    "            item_genre[i, k] = 1.0\n",
    "\n",
    "genre_count_mean_reciprocal = []\n",
    "for k in key_genre:\n",
    "    genre_count_mean_reciprocal.append(1.0 / genre_count[k])\n",
    "genre_count_mean_reciprocal = (np.array(genre_count_mean_reciprocal)).reshape((num_genre, 1))\n",
    "genre_error_weight = np.dot(item_genre, genre_count_mean_reciprocal)\n",
    "\n",
    "precision = np.zeros(4)\n",
    "recall = np.zeros(4)\n",
    "f1 = np.zeros(4)\n",
    "ndcg = np.zeros(4)\n",
    "RSP = np.zeros(4)\n",
    "REO = np.zeros(4)\n",
    "\n",
    "for i in range(20):\n",
    "    with tf.Session() as sess:\n",
    "        dpr = DPR_RSP(sess, args, train_df, vali_df, item_genre, genre_error_weight,\n",
    "                      key_genre, item_genre_list, user_genre_count)\n",
    "        [prec_one, rec_one, f_one, ndcg_one, Rec] = dpr.run()\n",
    "        [RSP_one, REO_one] = utility.ranking_analysis(Rec, vali_df, train_df, key_genre,\n",
    "                                                      item_genre_list, user_genre_count)\n",
    "        precision += prec_one\n",
    "        recall += rec_one\n",
    "        f1 += f_one\n",
    "        ndcg += ndcg_one\n",
    "        RSP += RSP_one\n",
    "        REO += REO_one\n",
    "\n",
    "with open('Rec_' + dataname + '_DPR_RSP.mat', \"wb\") as f:\n",
    "    np.save(f, Rec)\n",
    "\n",
    "\n",
    "precision /= n\n",
    "recall /= n\n",
    "f1 /= n\n",
    "ndcg /= n\n",
    "RSP /= n\n",
    "REO /= n\n",
    "\n",
    "print('')\n",
    "print('*' * 100)\n",
    "print('Averaged precision@1\\t%.7f,\\t||\\tprecision@5\\t%.7f,\\t||\\tprecision@10\\t%.7f,\\t||\\tprecision@15\\t%.7f' \\\n",
    "      % (precision[0], precision[1], precision[2], precision[3]))\n",
    "print('Averaged recall@1\\t%.7f,\\t||\\trecall@5\\t%.7f,\\t||\\trecall@10\\t%.7f,\\t||\\trecall@15\\t%.7f' \\\n",
    "      % (recall[0], recall[1], recall[2], recall[3]))\n",
    "print('Averaged f1@1\\t\\t%.7f,\\t||\\tf1@5\\t\\t%.7f,\\t||\\tf1@10\\t\\t%.7f,\\t||\\tf1@15\\t\\t%.7f' \\\n",
    "      % (f1[0], f1[1], f1[2], f1[3]))\n",
    "print('Averaged NDCG@1\\t\\t%.7f,\\t||\\tNDCG@5\\t\\t%.7f,\\t||\\tNDCG@10\\t\\t%.7f,\\t||\\tNDCG@15\\t\\t%.7f' \\\n",
    "      % (ndcg[0], ndcg[1], ndcg[2], ndcg[3]))\n",
    "print('*' * 100)\n",
    "print('Averaged RSP    @1\\t%.7f\\t||\\t@5\\t%.7f\\t||\\t@10\\t%.7f\\t||\\t@15\\t%.7f' \\\n",
    "      % (RSP[0], RSP[1], RSP[2], RSP[3]))\n",
    "print('Averaged REO @1\\t%.7f\\t||\\t@5\\t%.7f\\t||\\t@10\\t%.7f\\t||\\t@15\\t%.7f' \\\n",
    "      % (REO[0], REO[1], REO[2], REO[3]))\n",
    "print('*' * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea27fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = pd.read_pickle(r'ml1m-6/item_idd_genre_list.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58693ff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22880/3733755895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ml1m-6'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/training_df.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "pickle.load(open('./' + 'ml1m-6' + '/training_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f4e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d2c3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(r'ml1m-6/training_df.pkl')    \n",
    "vali_df = pd.read_pickle(r'ml1m-6/valiing_df.pkl')   # for validation\n",
    "# vali_df = pickle.load(open('./' + dataname + '/testing_df.pkl'))  # for testing\n",
    "key_genre = pd.read_pickle(r'ml1m-6/key_genre.pkl')  \n",
    "item_idd_genre_list = pd.read_pickle(r'ml1m-6/item_idd_genre_list.pkl')   \n",
    "genre_item_vector = pd.read_pickle(r'ml1m-6/genre_item_vector.pkl')    \n",
    "genre_count = pd.read_pickle(r'ml1m-6/genre_count.pkl')      \n",
    "user_genre_count = pd.read_pickle(r'ml1m-6/user_genre_count.pkl') \n",
    "\n",
    "num_item = len(train_df['item_id'].unique())\n",
    "num_user = len(train_df['user_id'].unique())\n",
    "num_genre = len(key_genre)\n",
    "\n",
    "item_genre_list = []\n",
    "for u in range(num_item):\n",
    "    gl = item_idd_genre_list[u]\n",
    "    tmp = []\n",
    "    for g in gl:\n",
    "        if g in key_genre:\n",
    "            tmp.append(g)\n",
    "    item_genre_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3880e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['user_id'].isin(vali_df['user_id'].unique())]\n",
    "train_df = train_df[train_df['item_id'].isin(vali_df['item_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "247c3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_list(df, user_size):\n",
    "    user_list = [list() for u in df['user_id'].unique()]\n",
    "    for row in df.itertuples():\n",
    "        user_list[row.user_id].append(row.item_id)\n",
    "    return user_list\n",
    "\n",
    "def create_pair(user_list):\n",
    "    pair = []\n",
    "    for user, item_list in enumerate(user_list):\n",
    "        pair.extend([(user, item) for item in item_list])\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aab75235",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_list = create_user_list(train_df, num_user)\n",
    "test_user_list = create_user_list(vali_df, num_user)\n",
    "train_pair = create_pair(train_user_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55303d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'user_size': num_user, 'item_size': num_item,\n",
    "            'train_user_list': train_user_list, 'test_user_list': test_user_list,\n",
    "            'train_pair': train_pair}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb0dfee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370139</th>\n",
       "      <td>6035</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370140</th>\n",
       "      <td>6035</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370141</th>\n",
       "      <td>6035</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370142</th>\n",
       "      <td>6035</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370143</th>\n",
       "      <td>6035</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370224</th>\n",
       "      <td>6035</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370225</th>\n",
       "      <td>6035</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370226</th>\n",
       "      <td>6035</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370227</th>\n",
       "      <td>6035</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370228</th>\n",
       "      <td>6035</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id\n",
       "370139     6035       48\n",
       "370140     6035      137\n",
       "370141     6035      625\n",
       "370142     6035      634\n",
       "370143     6035      198\n",
       "...         ...      ...\n",
       "370224     6035      240\n",
       "370225     6035      358\n",
       "370226     6035      507\n",
       "370227     6035      578\n",
       "370228     6035      583\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['user_id'] == 6035]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80d000b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6036"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2038a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.path.dirname(os.path.abspath('ml-1m-2.pickle'))\n",
    "os.makedirs(dirname, exist_ok=True)\n",
    "with open('ml-1m-2.pickle', 'wb') as f:\n",
    "    pickle.dump(dataset, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35b6a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_genre = np.zeros((num_item, num_genre))\n",
    "for i in range(num_item):\n",
    "    gl = item_genre_list[i]\n",
    "    for k in range(num_genre):\n",
    "        if key_genre[k] in gl:\n",
    "            item_genre[i, k] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9da8e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1481"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c95b97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6036"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "106e4a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1 for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700778f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
