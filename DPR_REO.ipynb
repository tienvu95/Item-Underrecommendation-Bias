{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4703ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import argparse\n",
    "import utility\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ae48f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DPR_REO:\n",
    "\n",
    "    def __init__(self, sess, args, train_df, vali_df, item_genre, genre_error_weight\n",
    "                 , key_genre, item_genre_list, user_genre_count):\n",
    "        self.dataname = args.dataname\n",
    "\n",
    "        self.layers = eval(args.layers)\n",
    "\n",
    "        self.key_genre = key_genre\n",
    "        self.item_genre_list = item_genre_list\n",
    "        self.user_genre_count = user_genre_count\n",
    "\n",
    "        self.sess = sess\n",
    "        self.args = args\n",
    "\n",
    "        self.num_cols = len(train_df['item_id'].unique())\n",
    "        self.num_rows = len(train_df['user_id'].unique())\n",
    "\n",
    "        self.hidden_neuron = args.hidden_neuron\n",
    "        self.neg = args.neg\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.vali_df = vali_df\n",
    "        self.num_train = len(self.train_df)\n",
    "        self.num_vali = len(self.vali_df)\n",
    "\n",
    "        self.train_epoch = args.train_epoch\n",
    "        self.train_epoch_a = args.train_epoch_a\n",
    "        self.display_step = args.display_step\n",
    "\n",
    "        self.lr_r = args.lr_r  # learning rate\n",
    "        self.lr_a = args.lr_a  # learning rate\n",
    "\n",
    "        self.reg = args.reg  # regularization term trade-off\n",
    "        self.reg_s = args.reg_s\n",
    "\n",
    "        self.num_genre = args.num_genre\n",
    "        self.alpha = args.alpha\n",
    "        self.item_genre = item_genre\n",
    "        self.genre_error_weight = genre_error_weight\n",
    "\n",
    "        self.genre_count_list = []\n",
    "        for k in range(self.num_genre):\n",
    "            self.genre_count_list.append(np.sum(item_genre[:, k]))\n",
    "\n",
    "        print('**********DPR_REO**********')\n",
    "        print(self.args)\n",
    "        self._prepare_model()\n",
    "\n",
    "\n",
    "    #load bpr checkpoint\n",
    "    def loadmodel(self, saver, checkpoint_dir):\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def run(self):\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "        saver = tf.compat.v1.train.Saver([self.P, self.Q])\n",
    "        self.loadmodel(saver, \"./\"+self.dataname+\"/BPR_check_points\")\n",
    "        print (\"restore done\")\n",
    "\n",
    "        for epoch_itr in range(1, self.train_epoch + 1 + self.train_epoch_a):\n",
    "            print(\"starting itr \" + str(epoch_itr))\n",
    "            self.train_model(epoch_itr)\n",
    "            if epoch_itr % self.display_step == 0:\n",
    "                self.test_model(epoch_itr)\n",
    "            print(\"end itr \" + str(epoch_itr))\n",
    "        return self.make_records()\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        with tf.name_scope(\"input_data\"):\n",
    "            # declare embedding u i j, genre to predict\n",
    "            self.user_input = tf.compat.v1.placeholder(tf.int32, shape=[None, 1], name=\"user_input\")\n",
    "            self.item_input_pos = tf.compat.v1.placeholder(tf.int32, shape=[None, 1], name=\"item_input_pos\")\n",
    "            self.item_input_neg = tf.compat.v1.placeholder(tf.int32, shape=[None, 1], name=\"item_input_neg\")\n",
    "\n",
    "            self.input_item_genre = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.num_genre]\n",
    "                                                   , name=\"input_item_genre\")\n",
    "            self.input_item_error_weight = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 1]\n",
    "                                                          , name=\"input_item_error_weight\")\n",
    "        #this is the part initialize BPR for MF\n",
    "        with tf.compat.v1.variable_scope(\"BPR\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            self.P = tf.compat.v1.get_variable(name=\"P\",\n",
    "                                     initializer=tf.compat.v1.truncated_normal(shape=[self.num_rows, self.hidden_neuron], mean=0,\n",
    "                                                                     stddev=0.03), dtype=tf.float32)\n",
    "            self.Q = tf.compat.v1.get_variable(name=\"Q\",\n",
    "                                     initializer=tf.compat.v1.truncated_normal(shape=[self.num_cols, self.hidden_neuron], mean=0,\n",
    "                                                                     stddev=0.03), dtype=tf.float32)\n",
    "        para_r = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=\"BPR\")\n",
    "\n",
    "\n",
    "        #adversary as a mlp\n",
    "        with tf.compat.v1.variable_scope(\"Adversarial\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            num_layer = len(self.layers)\n",
    "            adv_W = []\n",
    "            adv_b = []\n",
    "            for l in range(num_layer):\n",
    "                if l == 0:\n",
    "                    in_shape = 1\n",
    "                else:\n",
    "                    in_shape = self.layers[l - 1]\n",
    "                adv_W.append(tf.compat.v1.get_variable(name=\"adv_W\" + str(l),\n",
    "                                             initializer=tf.compat.v1.truncated_normal(shape=[in_shape, self.layers[l]],\n",
    "                                                                             mean=0, stddev=0.03), dtype=tf.float32))\n",
    "                adv_b.append(tf.compat.v1.get_variable(name=\"adv_b\" + str(l),\n",
    "                                             initializer=tf.compat.v1.truncated_normal(shape=[1, self.layers[l]],\n",
    "                                                                             mean=0, stddev=0.03), dtype=tf.float32))\n",
    "            adv_W_out = tf.compat.v1.get_variable(name=\"adv_W_out\",\n",
    "                                        initializer=tf.compat.v1.truncated_normal(shape=[self.layers[-1], self.num_genre],\n",
    "                                                                        mean=0, stddev=0.03), dtype=tf.float32)\n",
    "\n",
    "            adv_b_out = tf.compat.v1.get_variable(name=\"adv_b_out\",\n",
    "                                        initializer=tf.compat.v1.truncated_normal(shape=[1, self.num_genre],\n",
    "                                                                        mean=0, stddev=0.03), dtype=tf.float32)\n",
    "        para_a = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=\"Adversarial\")\n",
    "\n",
    "\n",
    "        ##prep embedding for bpr\n",
    "        p = tf.reduce_sum(tf.nn.embedding_lookup(self.P, self.user_input), 1)\n",
    "        q_neg = tf.reduce_sum(tf.nn.embedding_lookup(self.Q, self.item_input_neg), 1)\n",
    "        q_pos = tf.reduce_sum(tf.nn.embedding_lookup(self.Q, self.item_input_pos), 1)\n",
    "        ##prediction based on bpr embedding\n",
    "        predict_pos = tf.reduce_sum(p * q_pos, 1)\n",
    "        predict_neg = tf.reduce_sum(p * q_neg, 1)\n",
    "\n",
    "        r_cost1 = tf.reduce_sum(tf.nn.softplus(-(predict_pos - predict_neg))) #bpr cost\n",
    "        r_cost2 = self.reg * 0.5 * (self.l2_norm(self.P) + self.l2_norm(self.Q))  # regularization term\n",
    "        pred = tf.matmul(self.P, tf.transpose(self.Q))\n",
    "        self.s_mean = tf.reduce_mean(pred, axis=1)\n",
    "        self.s_std = tf.keras.backend.std(pred, axis=1)\n",
    "        self.s_cost = tf.reduce_sum(tf.square(self.s_mean) + tf.square(self.s_std) - 2 * tf.compat.v1.log(self.s_std) - 1)\n",
    "        self.r_cost = r_cost1 + r_cost2 + self.reg_s * 0.5 * self.s_cost\n",
    "\n",
    "\n",
    "        #last layer = prediction by the mlp adversary\n",
    "        adv_last = tf.reshape(predict_pos, [tf.shape(self.input_item_genre)[0], 1])\n",
    "        for l in range(num_layer):\n",
    "            adv = tf.nn.relu(tf.matmul(adv_last, adv_W[l]) + adv_b[l])\n",
    "            adv_last = adv #linear-relu\n",
    "        #sigmoid to convert output to prediction\n",
    "        self.adv_output = tf.nn.sigmoid(tf.matmul(adv_last, adv_W_out) + adv_b_out)\n",
    "\n",
    "        #adversarial cost (why they have the input_item_error_weight??)\n",
    "        self.a_cost = tf.reduce_sum(tf.square(self.adv_output - self.input_item_genre) * self.input_item_error_weight)\n",
    "\n",
    "        self.all_cost = self.r_cost - self.alpha * self.a_cost  # the loss function\n",
    "\n",
    "        #optimize for bpr, adversary and overall cost wrt to overall obj function\n",
    "        with tf.compat.v1.variable_scope(\"Optimizer\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            self.r_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr_r).minimize(self.r_cost, var_list=para_r)\n",
    "            self.a_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr_a).minimize(self.a_cost, var_list=para_a)\n",
    "            self.all_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr_r).minimize(self.all_cost, var_list=para_r)\n",
    "\n",
    "    def train_model(self, itr):\n",
    "        NS_start_time = time.time() * 1000.0\n",
    "        epoch_r_cost = 0.0\n",
    "        epoch_s_cost = 0.0\n",
    "        epoch_s_mean = 0.0\n",
    "        epoch_s_std = 0.0\n",
    "        epoch_a_cost = 0.0\n",
    "        num_sample, user_list, item_pos_list, item_neg_list = utility.negative_sample(self.train_df, self.num_rows,\n",
    "                                                                                      self.num_cols, self.neg)\n",
    "        NS_end_time = time.time() * 1000.0\n",
    "\n",
    "        start_time = time.time() * 1000.0\n",
    "        num_batch = int(num_sample / float(self.batch_size)) + 1\n",
    "        random_idx = np.random.permutation(num_sample)\n",
    "        for i in range(num_batch):\n",
    "            # get the indices of the current batch\n",
    "            if i == num_batch - 1:\n",
    "                batch_idx = random_idx[i * self.batch_size:]\n",
    "            elif i < num_batch - 1:\n",
    "                batch_idx = random_idx[(i * self.batch_size):((i + 1) * self.batch_size)]\n",
    "\n",
    "            if itr > self.train_epoch:\n",
    "                random_idx_a = np.random.permutation(num_sample)\n",
    "                print(\"start itr \" + str(itr) + \"for batch \" + str(i))\n",
    "                for j in range(num_batch):\n",
    "                    if j == num_batch - 1:\n",
    "                        batch_idx_a = random_idx_a[j * self.batch_size:]\n",
    "                    elif j < num_batch - 1:\n",
    "                        batch_idx_a = random_idx_a[(j * self.batch_size):((j + 1) * self.batch_size)]\n",
    "                    item_idx_list = ((item_pos_list[batch_idx_a, :]).reshape((len(batch_idx_a)))).tolist()\n",
    "                    _, tmp_a_cost = self.sess.run(  # do the optimization by the minibatch\n",
    "                        [self.a_optimizer, self.a_cost], #update the adversary\n",
    "                        feed_dict={self.user_input: user_list[batch_idx_a, :],\n",
    "                                   self.item_input_pos: item_pos_list[batch_idx_a, :],\n",
    "                                   self.item_input_neg: item_neg_list[batch_idx_a, :],\n",
    "                                   self.input_item_genre: self.item_genre[item_idx_list, :],\n",
    "                                   self.input_item_error_weight: self.genre_error_weight[item_idx_list, :]})\n",
    "                    epoch_a_cost += tmp_a_cost\n",
    "                    print(\"update epoch_a_cost done \" + str(j))\n",
    "                item_idx_list = ((item_pos_list[batch_idx, :]).reshape((len(batch_idx)))).tolist()\n",
    "                _, tmp_r_cost, tmp_s_cost, tmp_s_mean, tmp_s_std = self.sess.run(  # do the optimization by the minibatch\n",
    "                    [self.all_optimizer, self.all_cost, self.s_cost, self.s_mean, self.s_std], # update overall objective\n",
    "                    feed_dict={self.user_input: user_list[batch_idx, :],\n",
    "                               self.item_input_pos: item_pos_list[batch_idx, :],\n",
    "                               self.item_input_neg: item_neg_list[batch_idx, :],\n",
    "                               self.input_item_genre: self.item_genre[item_idx_list, :],\n",
    "                               self.input_item_error_weight: self.genre_error_weight[item_idx_list, :]})\n",
    "                epoch_r_cost += tmp_r_cost\n",
    "                epoch_s_mean += np.mean(tmp_s_mean)\n",
    "                epoch_s_std += np.mean(tmp_s_std)\n",
    "                epoch_s_cost += tmp_s_cost\n",
    "                print(\"update epoch_s_cost done\")\n",
    "            else:#for train_epoch, we only train the classifer (BPR) then append the adversarial training phase\n",
    "                item_idx_list = ((item_pos_list[batch_idx, :]).reshape((len(batch_idx)))).tolist()\n",
    "                _, tmp_r_cost, tmp_s_cost, tmp_s_mean, tmp_s_std = self.sess.run(  # do the optimization by the minibatch\n",
    "                    [self.r_optimizer, self.r_cost, self.s_cost, self.s_mean, self.s_std], #update the weight of the classifier\n",
    "                    feed_dict={self.user_input: user_list[batch_idx, :],\n",
    "                               self.item_input_pos: item_pos_list[batch_idx, :],\n",
    "                               self.item_input_neg: item_neg_list[batch_idx, :],\n",
    "                               self.input_item_genre: self.item_genre[item_idx_list, :],\n",
    "                               self.input_item_error_weight: self.genre_error_weight[item_idx_list, :]})\n",
    "                epoch_r_cost += tmp_r_cost\n",
    "                epoch_s_mean += np.mean(tmp_s_mean)\n",
    "                epoch_s_std += np.mean(tmp_s_std)\n",
    "                epoch_s_cost += tmp_s_cost\n",
    "        print(\"adjust a_cost\")                  \n",
    "        epoch_a_cost /= num_batch\n",
    "        if itr % self.display_step == 0:\n",
    "            print (\"Training //\", \"Epoch %d //\" % itr, \" Total r_cost = %.5f\" % epoch_r_cost,\n",
    "                   \" Total s_cost = %.5f\" % epoch_s_cost,\n",
    "                   \" Total s_mean = %.5f\" % epoch_s_mean,\n",
    "                   \" Total s_std = %.5f\" % epoch_s_std,\n",
    "                   \" Total a_cost = %.5f\" % epoch_a_cost,\n",
    "                   \"Training time : %d ms\" % (time.time() * 1000.0 - start_time),\n",
    "                   \"negative Sampling time : %d ms\" % (NS_end_time - NS_start_time),\n",
    "                   \"negative samples : %d\" % (num_sample))\n",
    "\n",
    "    def test_model(self, itr):  # calculate the cost and rmse of testing set in each epoch\n",
    "        if itr % self.display_step == 0:\n",
    "            start_time = time.time() * 1000.0\n",
    "            P, Q = self.sess.run([self.P, self.Q])\n",
    "            Rec = np.matmul(P, Q.T)\n",
    "\n",
    "            [precision, recall, f_score, NDCG] = utility.test_model_all(Rec, self.vali_df, self.train_df)\n",
    "            utility.ranking_analysis(Rec, self.vali_df, self.train_df, self.key_genre, self.item_genre_list,\n",
    "                                     self.user_genre_count)\n",
    "            print(\"testing computation done\")\n",
    "            avg_genre = []\n",
    "            for k in range(self.num_genre):\n",
    "                avg_genre.append(np.sum(Rec * self.item_genre[:, k].reshape((1, self.num_cols)))\n",
    "                                 / (self.genre_count_list[k] * self.num_rows))\n",
    "\n",
    "            std_avg_genre = np.std(avg_genre)\n",
    "            print (\n",
    "                \"Testing //\", \"Epoch %d //\" % itr,\n",
    "                \"Testing time : %d ms\" % (time.time() * 1000.0 - start_time))\n",
    "            print(\"avg rating genre: \" + str(avg_genre))\n",
    "            print(\"std of avg rating genre\" + str(std_avg_genre))\n",
    "            print(\"=\" * 200)\n",
    "\n",
    "    def make_records(self):  # record all the results' details into files\n",
    "        P, Q = self.sess.run([self.P, self.Q])\n",
    "        Rec = np.matmul(P, Q.T)\n",
    "\n",
    "        [precision, recall, f_score, NDCG] = utility.test_model_all(Rec, self.vali_df, self.train_df)\n",
    "        return precision, recall, f_score, NDCG, Rec\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_norm(tensor):\n",
    "        return tf.reduce_sum(tf.square(tensor))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5c09210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='DPR_REO')\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('--train_epoch', type=int, default=0)\n",
    "parser.add_argument('--train_epoch_a', type=int, default=20)\n",
    "parser.add_argument('--display_step', type=int, default=1)\n",
    "parser.add_argument('--lr_r', type=float, default=0.01)\n",
    "parser.add_argument('--lr_a', type=float, default=0.005)\n",
    "parser.add_argument('--reg', type=float, default=0.1)\n",
    "parser.add_argument('--reg_s', type=float, default=30)\n",
    "parser.add_argument('--hidden_neuron', type=int, default=20)\n",
    "parser.add_argument('--n', type=int, default=1)\n",
    "parser.add_argument('--neg', type=int, default=5)\n",
    "parser.add_argument('--alpha', type=float, default=1000.0)\n",
    "parser.add_argument('--batch_size', type=int, default=1024)\n",
    "parser.add_argument('--layers', nargs='?', default='[50, 50, 50, 50]')\n",
    "parser.add_argument('--dataname', nargs='?', default='ml1m-6')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6d483b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(r'ml1m-6/training_df.pkl')\n",
    "vali_df = pd.read_pickle(r'ml1m-6/valiing_df.pkl')   # for validation\n",
    "# vali_df = pickle.load(open('./' + dataname + '/testing_df.pkl'))  # for testing\n",
    "key_genre = pd.read_pickle(r'ml1m-6/key_genre.pkl')\n",
    "item_idd_genre_list = pd.read_pickle(r'ml1m-6/item_idd_genre_list.pkl')\n",
    "genre_item_vector = pd.read_pickle(r'ml1m-6/genre_item_vector.pkl')\n",
    "genre_count = pd.read_pickle(r'ml1m-6/genre_count.pkl')\n",
    "user_genre_count = pd.read_pickle(r'ml1m-6/user_genre_count.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4e58a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "number of positive feedback: 152524\n",
      "estimated number of training samples: 762620\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_item = len(train_df['item_id'].unique())\n",
    "num_user = len(train_df['user_id'].unique())\n",
    "num_genre = len(key_genre)\n",
    "\n",
    "item_genre_list = []\n",
    "for u in range(num_item):\n",
    "    gl = item_idd_genre_list[u]\n",
    "    tmp = []\n",
    "    for g in gl:\n",
    "        if g in key_genre:\n",
    "            tmp.append(g)\n",
    "    item_genre_list.append(tmp)\n",
    "\n",
    "print('!' * 100)\n",
    "print('number of positive feedback: ' + str(len(train_df)))\n",
    "print('estimated number of training samples: ' + str(args.neg * len(train_df)))\n",
    "print('!' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e3e94ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_genre = np.zeros((num_item, num_genre))\n",
    "for i in range(num_item):\n",
    "    gl = item_genre_list[i]\n",
    "    for k in range(num_genre):\n",
    "        if key_genre[k] in gl:\n",
    "            item_genre[i, k] = 1.0\n",
    "\n",
    "genre_count_mean_reciprocal = []\n",
    "\n",
    "##there are six key_genre --> in the training dataset, count the number of movies for each genre\n",
    "#genre_count = dictionary with number of movies for each keygrenre\n",
    "for k in key_genre:\n",
    "    genre_count_mean_reciprocal.append(1.0 / genre_count[k])\n",
    "genre_count_mean_reciprocal = (np.array(genre_count_mean_reciprocal)).reshape((num_genre, 1))\n",
    "genre_error_weight = np.dot(item_genre, genre_count_mean_reciprocal)\n",
    "\n",
    "args.num_genre = num_genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "54d0117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********DPR_REO**********\n",
      "Namespace(f='/home/vuhoang181/.local/share/jupyter/runtime/kernel-2aaa66f4-45f7-4b84-8d88-9ed21cb2aec6.json', train_epoch=0, train_epoch_a=20, display_step=1, lr_r=0.01, lr_a=0.005, reg=0.1, reg_s=30, hidden_neuron=20, n=1, neg=5, alpha=1000.0, batch_size=1024, layers='[50, 50, 50, 50]', dataname='ml1m-2', num_genre=2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 6 and 2 for '{{node sub_77}} = Sub[T=DT_FLOAT](Sigmoid_15, input_data_16/input_item_genre)' with input shapes: [?,6], [?,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36392/262190402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         dpr = DPR_REO(sess, args, train_df, vali_df, item_genre, genre_error_weight,\n\u001b[0m\u001b[1;32m     14\u001b[0m                       key_genre, item_genre_list, user_genre_count)\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mprec_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcg_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRec\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36392/3430785357.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, args, train_df, vali_df, item_genre, genre_error_weight, key_genre, item_genre_list, user_genre_count)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'**********DPR_REO**********'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36392/3430785357.py\u001b[0m in \u001b[0;36m_prepare_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m#adversarial cost (why they have the input_item_error_weight??)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madv_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_item_genre\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_item_error_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_cost\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_cost\u001b[0m  \u001b[0;31m# the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1965\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m   \u001b[0;31m# Record the current Python stack trace as the creating stacktrace of this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 6 and 2 for '{{node sub_77}} = Sub[T=DT_FLOAT](Sigmoid_15, input_data_16/input_item_genre)' with input shapes: [?,6], [?,2]."
     ]
    }
   ],
   "source": [
    "precision = np.zeros(4)\n",
    "recall = np.zeros(4)\n",
    "f1 = np.zeros(4)\n",
    "ndcg = np.zeros(4)\n",
    "RSP = np.zeros(4)\n",
    "REO = np.zeros(4)\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "n = args.n\n",
    "for i in range(n):\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        dpr = DPR_REO(sess, args, train_df, vali_df, item_genre, genre_error_weight,\n",
    "                      key_genre, item_genre_list, user_genre_count)\n",
    "        [prec_one, rec_one, f_one, ndcg_one, Rec] = dpr.run()\n",
    "        [RSP_one, REO_one] = utility.ranking_analysis(Rec, vali_df, train_df, key_genre, item_genre_list,\n",
    "                                                      user_genre_count)\n",
    "        precision += prec_one\n",
    "        recall += rec_one\n",
    "        f1 += f_one\n",
    "        ndcg += ndcg_one\n",
    "        RSP += RSP_one\n",
    "        REO += REO_one\n",
    "\n",
    "with open('Rec_' + dataname + '_DPR_REO.mat', \"wb\") as f:\n",
    "    np.save(f, Rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894798eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision /= n\n",
    "recall /= n\n",
    "f1 /= n\n",
    "ndcg /= n\n",
    "RSP /= n\n",
    "REO /= n\n",
    "\n",
    "print('')\n",
    "print('*' * 100)\n",
    "print('Averaged precision@1\\t%.7f,\\t||\\tprecision@5\\t%.7f,\\t||\\tprecision@10\\t%.7f,\\t||\\tprecision@15\\t%.7f' \\\n",
    "      % (precision[0], precision[1], precision[2], precision[3]))\n",
    "print('Averaged recall@1\\t%.7f,\\t||\\trecall@5\\t%.7f,\\t||\\trecall@10\\t%.7f,\\t||\\trecall@15\\t%.7f' \\\n",
    "      % (recall[0], recall[1], recall[2], recall[3]))\n",
    "print('Averaged f1@1\\t\\t%.7f,\\t||\\tf1@5\\t\\t%.7f,\\t||\\tf1@10\\t\\t%.7f,\\t||\\tf1@15\\t\\t%.7f' \\\n",
    "      % (f1[0], f1[1], f1[2], f1[3]))\n",
    "print('Averaged NDCG@1\\t\\t%.7f,\\t||\\tNDCG@5\\t\\t%.7f,\\t||\\tNDCG@10\\t\\t%.7f,\\t||\\tNDCG@15\\t\\t%.7f' \\\n",
    "      % (ndcg[0], ndcg[1], ndcg[2], ndcg[3]))\n",
    "print('*' * 100)\n",
    "print('Averaged RSP    @1\\t%.7f\\t||\\t@5\\t%.7f\\t||\\t@10\\t%.7f\\t||\\t@15\\t%.7f' \\\n",
    "      % (RSP[0], RSP[1], RSP[2], RSP[3]))\n",
    "print('Averaged REO @1\\t%.7f\\t||\\t@5\\t%.7f\\t||\\t@10\\t%.7f\\t||\\t@15\\t%.7f' \\\n",
    "      % (REO[0], REO[1], REO[2], REO[3]))\n",
    "print('*' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bb46e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 % 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48113f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
